{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72bfce9b",
   "metadata": {},
   "source": [
    "# 개요\n",
    "> 본 노트북 코드는 **통근 OD 데이터**를 산출하기 이전에 **100명**의 랜덤 샘플을 두고 몇 퍼센트의 통근 패턴을 감지할 수 있는 지를 기반으로 로직의 합당성을 판단하는 로직임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d609a824605bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb as duck\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from scipy.spatial import Voronoi\n",
    "from shapely.geometry import Polygon\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0b0659632fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duckdb db 불러오기\n",
    "con = duck.connect(database='myanalysis.db', read_only=False)\n",
    "con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db3e14b03a9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정류장 정보 들고 오기\n",
    "station = pd.read_csv('import_data/TB_KTS_STTN/202406/TB_KTS_STTN_20240603.csv')\n",
    "station07 = pd.read_csv('import_data/TB_KTS_STTN/202407/TB_KTS_STTN_20240701.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b14619efd1ff1",
   "metadata": {},
   "source": [
    "# 1. 2024년 6월 샘플 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759cf9a07cd285b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평일만 불러오기- 2024년 6월\n",
    "dates = pd.date_range(start = '2024-06-01', end = '2024-06-30', freq='D')\n",
    "weekday = dates[dates.weekday < 5]\n",
    "weekday_strs = weekday.strftime('%Y%m%d').to_list()\n",
    "card_sets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263196228e336130",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = f'''\n",
    "    SELECT DISTINCT 가상카드번호\n",
    "    FROM read_csv('import_data/TB_KTS_DWTCD_METROPOLITAN/202406/TB_KTS_DWTCD_METROPOLITAN_20240603.csv', columns ={{'가상카드번호':'VARCHAR'}})\n",
    "    '''\n",
    "cards = set(con.execute(q).fetchdf()['가상카드번호'])\n",
    "sampled = random.sample(list(cards),100)\n",
    "print(len(sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a15101a263637",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = pd.DataFrame(sampled)[0].str.split(',').str[2]\n",
    "sample_ids = sample_ids.to_list()\n",
    "sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67825b11d14a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db에 insert\n",
    "con.execute(\"\"\"CREATE OR REPLACE TEMP TABLE cardlist\n",
    "                (card_id VARCHAR)\"\"\")\n",
    "for sample_id in sample_ids:\n",
    "    con.execute(\"INSERT INTO cardlist VALUES (?)\", [sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0a339e69e7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100명에 대한 6월 한달 데이터 가져오기\n",
    "dfs = []\n",
    "single_card = sampled[0]\n",
    "for day in weekday_strs:\n",
    "    q = f'''\n",
    "    SELECT *\n",
    "    FROM read_csv('import_data/TB_KTS_DWTCD_METROPOLITAN/202406/TB_KTS_DWTCD_METROPOLITAN_{day}.csv', types= {{'정산사노선ID':'VARCHAR', '가상카드번호':'VARCHAR'}})\n",
    "    WHERE 가상카드번호 IN (SELECT card_id FROM cardlist)\n",
    "    '''\n",
    "    df = con.query(q).to_df()\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2ca57e15dc42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_month_df = pd.concat(dfs, ignore_index=True)\n",
    "# full_month_df.columns = ['운행일자', '정산사ID', '가상카드번호', '정산지역코드', '카드구분코드', '정산사차량ID', '교통수단코드',\n",
    "#        '정산사노선ID', '승차일시', '정산사승차정류장ID', '하차일시', '정산사하차정류장ID', '트랜잭션ID', '환승건수',\n",
    "#        '이용자유형코드(시스템)', '이용자수', '이용거리', '탑승시간']\n",
    "print(full_month_df.가상카드번호.nunique())\n",
    "full_month_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69dcf7dc3d9265",
   "metadata": {},
   "source": [
    "## 1.1. 24년 6월 샘플 데이터 저장 & 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8a56ed4f460c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_month_df.to_csv('sampled_202406.csv')\n",
    "full_month_df = pd.read_csv('sampled_202406.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56be52d9e23dc10",
   "metadata": {},
   "source": [
    "# 2. 목적통행으로 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70397a3af2b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교통수단구분 & 정산지역코드 타입 정리\n",
    "full_month_df['교통수단구분'] = full_month_df['교통수단코드'].apply(lambda x: 'T' if (199<x & x<300) else 'B')\n",
    "full_month_df.정산지역코드 = full_month_df.정산지역코드.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45bc3f85da068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 집계 방식 정의\n",
    "\n",
    "agg_dict = {**{x : 'first' for x in ['정산사ID', '이용자유형코드(시스템)']},\n",
    "            **{x : 'sum' for x in ['이용거리', '탑승시간']},\n",
    "           **{x: 'first' for x in ['정산사승차정류장ID', '승차일시']},\n",
    "           **{x: 'last' for x in ['정산사하차정류장ID', '하차일시']}}\n",
    "agg_dict['환승건수'] = 'max'\n",
    "# multi-lines\n",
    "multi_lines = {}\n",
    "multi_lines['승차정산지역코드'] = ('정산지역코드', 'first')\n",
    "multi_lines['하차정산지역코드'] = ('정산지역코드', 'last')\n",
    "multi_lines['승차교통수단구분'] = ('교통수단구분', 'first')\n",
    "multi_lines['하차교통수단구분'] = ('교통수단구분', 'last')\n",
    "multi_lines['승차노선ID'] = ('정산사노선ID', 'first')\n",
    "multi_lines['하차노선ID'] = ('정산사노선ID', 'last')\n",
    "\n",
    "print(agg_dict)\n",
    "multi_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510ffba4f54267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목적통행으로 집계  4037 -> 2614건으로\n",
    "full_month_df.sort_values(['운행일자', '승차일시'], inplace=True)\n",
    "main_trip = full_month_df.groupby(['운행일자', '가상카드번호', '트랜잭션ID']).agg(agg_dict).reset_index()\n",
    "support_trip = full_month_df.groupby(['운행일자', '가상카드번호', '트랜잭션ID']).agg(**multi_lines).reset_index()\n",
    "linked_202406_100 = pd.concat([main_trip, support_trip.iloc[:, 2:]], axis=1)\n",
    "linked_202406_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62007f94048d83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상카드번호에 대한 일별 목적 통행량에 대한 기초통계\n",
    "daily_counts = linked_202406_100.groupby(['운행일자', '가상카드번호']).size().reset_index(name= 'day_counts')\n",
    "daily_counts.day_counts.value_counts().reset_index().sort_values('day_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c216d84ed0f252c",
   "metadata": {},
   "source": [
    "# 3. 케이스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aee83da4084cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석을 위한 데이터 전처리\n",
    "# 1. 정류장ID -> int\n",
    "linked_202406_100['정산사승차정류장ID'] = pd.to_numeric(linked_202406_100['정산사승차정류장ID'], errors='coerce')\n",
    "linked_202406_100['정산사하차정류장ID'] = pd.to_numeric(linked_202406_100['정산사하차정류장ID'], errors='coerce')\n",
    "\n",
    "# 2. 데이터프레임에 요일 붙이기\n",
    "linked_202406_100['운행일자'] = pd.to_datetime(linked_202406_100['운행일자'].astype(str), format='%Y%m%d')\n",
    "linked_202406_100['요일'] = linked_202406_100['운행일자'].dt.day_name()\n",
    "\n",
    "# 3. 현충일(공휴일) 제외\n",
    "linked_202406_100 = linked_202406_100[~(linked_202406_100.운행일자 =='2024-06-06')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34e1fb6a1ef6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_202406_100.가상카드번호.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00179b28477ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_202406_100['이용자유형코드(시스템)'].value_counts()\n",
    "# 1\t일반인\n",
    "# 2\t어린이\n",
    "# 3\t청소년\n",
    "# 4\t경로\n",
    "# 5\t장애인\n",
    "# 6\t국가유공자\n",
    "# 7\t외국인\n",
    "# 8\t기타"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108059614bfa02e5",
   "metadata": {},
   "source": [
    "# 4. 케이스 분류 로직 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894a28bf0564b9b",
   "metadata": {},
   "source": [
    "## 4.1. 지하철 기준 보로노이 경계 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903f1adc0b773f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3:30-4:00 1차 시도\n",
    "# 4:00-5:40\n",
    "\n",
    "# 지하철 정류장 보로노이 패턴 생성\n",
    "subway = station[station['교통수단구분'] == 'T']\n",
    "# subway[subway.duplicated(subset=['지역코드', '정류장명칭'], keep=False)].sort_values('정류장명칭') # 1158개 중 230개가 중복, 즉 115개 드랍 가능\n",
    "# 호선이 다르지만 이름은 같은 경우 -> 하나의 좌표로 생성하도록\n",
    "subway_no_dupe = subway.drop_duplicates(subset=['지역코드', '정류장명칭'], keep='first')\n",
    "\n",
    "subway_pts = np.vstack([subway_no_dupe.정류장GPSX좌표, subway_no_dupe.정류장GPSY좌표]).T\n",
    "\n",
    "vor_subway = Voronoi(subway_pts)\n",
    "vor_subway\n",
    "\n",
    "polys = []\n",
    "for region in vor_subway.regions:\n",
    "    if not region or -1 in region:\n",
    "        continue\n",
    "    verts = [vor_subway.vertices[i] for i in region]\n",
    "    poly = Polygon(verts)\n",
    "    polys.append(poly)\n",
    "voronoi_subway = gpd.GeoDataFrame(geometry=polys, crs = 'EPSG:5179')\n",
    "voronoi_subway = voronoi_subway.reset_index().rename(columns={'index':'cluster_id'})\n",
    "voronoi_subway.to_csv('voronoi_subway.csv')\n",
    "voronoi_subway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c830519599eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station에 클러스터ID 붙이기\n",
    "# 지방에는 지하철이 많이 없어서 NULL 값 발생\n",
    "station_gdf = gpd.GeoDataFrame(station, geometry = gpd.points_from_xy(station.정류장GPSX좌표, station.정류장GPSY좌표))\n",
    "station_labeled = station_gdf.sjoin(voronoi_subway, how='left', predicate='within').drop('index_right', axis=1)\n",
    "print(len(station_labeled))\n",
    "station_labeled.isnull().sum()\n",
    "# station_labeled.to_csv('station_labeled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc37fb7ac93cc85e",
   "metadata": {},
   "source": [
    "## 4.2. 최빈값 로직 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7902ee8d5d50f",
   "metadata": {},
   "source": [
    "### 4.2.1. 클러스터ID 붙이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede144d6aad29836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터id 붙이기\n",
    "cluster_202406_b = linked_202406_100.merge(station_labeled[['정류장ID', '정류장명칭', '지역코드','교통수단구분', '정류장GPSX좌표', '정류장GPSY좌표', 'cluster_id']],\n",
    "                               how='left',\n",
    "                                left_on =['승차정산지역코드','승차교통수단구분', '정산사승차정류장ID'],\n",
    "                                right_on = ['지역코드','교통수단구분', '정류장ID']).drop(columns=['정류장ID', '지역코드', '교통수단구분']).rename(columns ={'cluster_id':'승차클러스터ID',\n",
    "                                                                                                                                   '정류장명칭':'승차정류장명칭',\n",
    "                                                                                                                                            '정류장GPSX좌표':'승차정류장_위도',\n",
    "                                                                                                                                 '정류장GPSY좌표':'승차정류장_경도'})\n",
    "# test._merge.value_counts()\n",
    "cluster_202406 = cluster_202406_b.merge(station_labeled[['정류장ID', '정류장명칭', '지역코드','교통수단구분', '정류장GPSX좌표', '정류장GPSY좌표', 'cluster_id']],\n",
    "                               how='left', left_on =['하차정산지역코드','하차교통수단구분', '정산사하차정류장ID'],\n",
    "                               right_on = ['지역코드','교통수단구분', '정류장ID']).drop(columns=['정류장ID', '지역코드', '교통수단구분']).rename(columns ={'cluster_id':'하차클러스터ID',\n",
    "                                                                                                                                   '정류장명칭':'하차정류장명칭',\n",
    "                                                                                                                                            '정류장GPSX좌표':'하차정류장_위도',\n",
    "                                                                                                                                 '정류장GPSY좌표':'하차정류장_경도'})\n",
    "# test._merge.value_counts()\n",
    "# 승하차일시 datetime 형식으로 변경\n",
    "cluster_202406['승차일시'] = pd.to_datetime(cluster_202406['승차일시'], format ='%Y%m%d%H%M%S')\n",
    "cluster_202406['하차일시'] = pd.to_datetime(cluster_202406['하차일시'], format ='%Y%m%d%H%M%S')\n",
    "cluster_202406 = cluster_202406.sort_values('승차일시')\n",
    "cluster_202406.가상카드번호.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044ebdef3ab41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "station[station.정류장ID == \t1270\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a6654a3ed9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케이스: 전체 통행건수가 적으며 랜덤한 이동. 8 달에 10번 미만\n",
    "# 케이스: 통행건수가 많으나 목적지나 시간이 랜덤한 경우 5\n",
    "# 케이스4: 12~16시 업무지에서 출발하는 케이스라 업무지가 산출되지 않는 케이스 5\n",
    "# 케이스3: 일반적 통근패턴이나 내리는 곳이 거리가 있는 경우: ex. 서원역(주거지), 신림역 등 번갈아가면서 이용 1\n",
    "# 케이스5: 두개의 출근지를 가짐- 하루는 홍대, 하루는 면목동으로 가는 경우 업무지 빈도값 불일치함 3\n",
    "# 케이스6: 비추적 이동패턴이 섞인 경우- 주거지만 추정됨 1\n",
    "# 케이스8: 하차정류장 Null값  1\n",
    "# 케이스9: 귀가만 일정하게 하는 패턴 2\n",
    "# 케이스10: 동네 방황형 2: 이용거리 중위값 1500m 미만\n",
    "test_1 = cluster_202406[cluster_202406['가상카드번호'] == 'Yalk8Ur29Q98R017C2lxQtlgoiuv+cm8EHgSfN6NMb4=']\n",
    "test_1.이용거리.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341de6da4b88da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1.value_counts(['승차클러스터ID', '하차클러스터ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a239c213483a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주거지 테스트 오전 승차 정류장최빈값, 오후 하차 정류장 최빈값\n",
    "print('<06~12시 기준 출발 정류장 빈도>', test_1[(6<=test_1['승차일시'].dt.hour)&(test_1['승차일시'].dt.hour<12)].승차클러스터ID.value_counts(),'\\n')\n",
    "print('<16~24시 기준 도착 정류장 빈도>', test_1[(16<=test_1['하차일시'].dt.hour)&(test_1['하차일시'].dt.hour<24)].하차클러스터ID.value_counts(),'\\n')\n",
    "\n",
    "# 업무지 테스트 오전 하차 정류장 최빈값, 오후 승차 정류장 최빈값\n",
    "print('<06~12시 기준 도착 정류장 빈도>', test_1[(6<=test_1['하차일시'].dt.hour)&(test_1['하차일시'].dt.hour<12)].하차클러스터ID.value_counts(),'\\n')\n",
    "print('<16~24시 기준 출발 정류장 빈도>', test_1[(16<=test_1['승차일시'].dt.hour)&(test_1['승차일시'].dt.hour<24)].승차클러스터ID.value_counts())\n",
    "# test_1[(test_1['승차일시'].dt.hour>6)&(test_1['승차일시'].dt.hour<12)].승차클러스터ID.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88f3d0cb49d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 개인 기준으로 최빈값 도출해보기\n",
    "# def commuting_mode(df):\n",
    "#     # df 최빈값 일치할 시에만\n",
    "#     live_cluster = None\n",
    "#     work_cluster = None\n",
    "#     # 주거지 산출 - 오전 승차&오후 하차\n",
    "#     board_am_mask = (6<=df['승차일시'].dt.hour)&(df['승차일시'].dt.hour<12)\n",
    "#     alight_pm_mask = (16<=df['하차일시'].dt.hour)&(df['하차일시'].dt.hour<24)\n",
    "\n",
    "#     # 업무지 산출 - 오전 하차&오후 승차\n",
    "#     alight_am_mask = (6<=df['하차일시'].dt.hour)&(df['하차일시'].dt.hour<12)\n",
    "#     board_pm_mask = (16<=df['승차일시'].dt.hour)&(df['승차일시'].dt.hour<24)\n",
    "\n",
    "\n",
    "#     # 예상 주거지 로직 - 오전 승차 & 오후 하차 클러스터 최빈값 일치유무\n",
    "#     try:\n",
    "#         if df[board_am_mask].승차클러스터ID.mode().iloc[0] == df[alight_pm_mask].하차클러스터ID.mode().iloc[0]:\n",
    "#             live_cluster = df[board_am_mask].승차클러스터ID.mode().iloc[0]\n",
    "#     except Exception as e:\n",
    "#         None\n",
    "\n",
    "#     # 예상 업무지 로직 - 최빈값 일치유무\n",
    "#     try:\n",
    "#         if df[board_pm_mask].승차클러스터ID.mode().iloc[0] == df[alight_am_mask].하차클러스터ID.mode().iloc[0]:\n",
    "#             work_cluster = df[alight_am_mask].하차클러스터ID.mode().iloc[0]\n",
    "#     except Exception as e:\n",
    "#         None\n",
    "\n",
    "#     return pd.DataFrame({\n",
    "#     'card_id':[df.iloc[0]['가상카드번호']],\n",
    "#     'home_cluster':[live_cluster],\n",
    "#     'work_cluster':[work_cluster],\n",
    "#     'under_10': len(df)<10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0d473f449d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 함수 적용\n",
    "# test_cluster = cluster_202406.groupby('가상카드번호').apply(commuting_mode).reset_index(drop=True)\n",
    "# # cluster_202406.groupby('가상카드번호').apply(test_mode).reset_index(drop=True)\n",
    "# commute_cluster = test_cluster[(~test_cluster.home_cluster.isna())&(~test_cluster.work_cluster.isna())]\n",
    "# test_cards = list(commute_cluster.card_id.unique())\n",
    "# print('주거&퇴근클러스터 존재하는 인원 수: ', len(commute_cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3ffad1d205b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 68인의 미분류 케이스\n",
    "# test_cluster[test_cluster.card_id == '3PXKELFhzI5unFXoRWFAlnI4mehBy21RMEtQxrEiK94=']\n",
    "# test_cluster\n",
    "# set(cluster_202406.가상카드번호.unique()) - set(test_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590a708a6fdd5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개인 기준으로 최빈값 도출해보기\n",
    "# 평균 이동시간, 이동거리\n",
    "\n",
    "\n",
    "def commuting_mode(df):\n",
    "    # df 최빈값 일치할 시에만\n",
    "    live_cluster = None\n",
    "    work_cluster = None\n",
    "    # 주거지 산출 - 오전 승차&오후 하차\n",
    "    board_am_mask = (6<=df['승차일시'].dt.hour)&(df['승차일시'].dt.hour<12)\n",
    "    alight_pm_mask = (16<=df['하차일시'].dt.hour)&(df['하차일시'].dt.hour<24)\n",
    "\n",
    "    # 업무지 산출 - 오전 하차&오후 승차\n",
    "    alight_am_mask = (6<=df['하차일시'].dt.hour)&(df['하차일시'].dt.hour<12)\n",
    "    board_pm_mask = (16<=df['승차일시'].dt.hour)&(df['승차일시'].dt.hour<24)\n",
    "\n",
    "\n",
    "    # 예상 주거지 로직 - 오전 승차 & 오후 하차 클러스터 최빈값 일치유무\n",
    "    try:\n",
    "        if df[board_am_mask].승차클러스터ID.mode().iloc[0] == df[alight_pm_mask].하차클러스터ID.mode().iloc[0]:\n",
    "            live_cluster = df[board_am_mask].승차클러스터ID.mode().iloc[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        None\n",
    "\n",
    "    # 예상 업무지 로직 - 최빈값 일치유무\n",
    "    try:\n",
    "        if df[board_pm_mask].승차클러스터ID.mode().iloc[0] == df[alight_am_mask].하차클러스터ID.mode().iloc[0]:\n",
    "            work_cluster = df[alight_am_mask].하차클러스터ID.mode().iloc[0]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        None\n",
    "\n",
    "    # 예상 주거지 정류장 찾기 ( L-> W 출근 케이스만 필터하여 수행)\n",
    "    sub_df = df[(df.승차클러스터ID == live_cluster)&(df.하차클러스터ID == work_cluster)]\n",
    "    live_sub_df = sub_df.groupby(['승차정산지역코드', '승차교통수단구분', '정산사승차정류장ID', '승차정류장명칭',\n",
    "                                 '하차정산지역코드', '하차교통수단구분', '정산사하차정류장ID', '하차정류장명칭',\n",
    "                                  # '승차정류장_위도', '승차정류장_경도','하차정류장_위도', '하차정류장_경도'\n",
    "                                 ]).agg(to_work_frequency =('정산사승차정류장ID', 'size'),\n",
    "                                        average_to_work_travel_time = ('탑승시간', 'mean'), average_to_work_travel_distance = ('이용거리', 'mean')).reset_index().rename(columns=\n",
    "                                                                                      {'정산사승차정류장ID': '주거지정류장ID',\n",
    "                                                                                       '정산사하차정류장ID': '업무지정류장ID',\n",
    "                                                                                      '승차정류장명칭': '주거지정류장명칭',\n",
    "                                                                                      '하차정류장명칭': '업무지정류장명칭',\n",
    "                                                                                      '승차교통수단구분': '출발교통수단구분',\n",
    "                                                                                       '하차교통수단구분': '도착교통수단구분',\n",
    "                                                                                      '승차정산지역코드':'출발정산지역코드',\n",
    "                                                                                      '하차정산지역코드':'도착정산지역코드'})\n",
    "\n",
    "\n",
    "    # 예상 업무지 정류장 찾기 ( L <- W 퇴근 케이스)\n",
    "    sub_df = df[(df.승차클러스터ID == work_cluster)&(df.하차클러스터ID == live_cluster)]\n",
    "    work_sub_df = sub_df.groupby(['승차정산지역코드', '승차교통수단구분', '정산사승차정류장ID', '승차정류장명칭',\n",
    "                                   '하차정산지역코드', '하차교통수단구분', '정산사하차정류장ID', '하차정류장명칭',\n",
    "                                  # '승차정류장_위도', '승차정류장_경도', '하차정류장_위도', '하차정류장_경도'\n",
    "                                 ]).agg(to_live_frequency =('정산사하차정류장ID', 'size'),\n",
    "                                        average_to_live_travel_time = ('탑승시간', 'mean'), average_to_live_travel_distance = ('이용거리', 'mean')).reset_index().rename(columns=\n",
    "                                                                                      {'정산사승차정류장ID': '업무지정류장ID', # 업무지 -> 주거지로 출발지<-> 도착지, 등 순서를 바꿔서 적용\n",
    "                                                                                       '정산사하차정류장ID': '주거지정류장ID', # Merge로 한 줄로 합치기 위함\n",
    "                                                                                      '승차정류장명칭': '업무지정류장명칭',\n",
    "                                                                                      '하차정류장명칭': '주거지정류장명칭',\n",
    "                                                                                      '승차교통수단구분': '도착교통수단구분',\n",
    "                                                                                       '하차교통수단구분': '출발교통수단구분',\n",
    "                                                                                      '승차정산지역코드':'도착정산지역코드',\n",
    "                                                                                      '하차정산지역코드':'출발정산지역코드'})\n",
    "    live_work_df = live_sub_df.merge(work_sub_df, how = 'outer', on = ['주거지정류장ID', '업무지정류장ID',\n",
    "                                                                      '출발정산지역코드', '출발교통수단구분','주거지정류장명칭',\n",
    "                                                                     '도착정산지역코드', '도착교통수단구분', '업무지정류장명칭'])\n",
    "    return live_work_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3486648488c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 적용\n",
    "test_cluster = cluster_202406.groupby('가상카드번호').apply(commuting_mode).reset_index(drop=True)\n",
    "test_cluster\n",
    "test_cluster.to_csv('_output/sample_100_live_work.csv', encoding='euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c24056bf2563fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정류장 좌표 정보 붙이기\n",
    "test_100_station = test_cluster.merge(station[['지역코드', '교통수단구분', '정류장ID', '정류장명칭', '정류장GPSY좌표', '정류장GPSX좌표']], how='left',\n",
    "                                      left_on = ['expect_home_station_id', '승차정산지역코드', '승차교통수단구분'],\n",
    "                                       right_on = ['정류장ID', '지역코드', '교통수단구분'],\n",
    "                                      indicator=True)\n",
    "print(test_100_station._merge.value_counts())\n",
    "test_100_station"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3.11]",
   "language": "python",
   "name": "conda-env-python3.11-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
