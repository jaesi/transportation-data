"""
# êµí†µì¹´ë“œ í†µê·¼OD ë°ì´í„° í›„ì²˜ë¦¬ DAG
- í•´ë‹¹ dagí”„ë¡œì„¸ìŠ¤ëŠ” ë°ì´í„° ì•ˆì‹¬êµ¬ì—­ì—ì„œ ë°˜ì¶œ í›„ S3ì— ì—…ë¡œë“œëœ ì›ë³¸ í†µê·¼OD Parquet íŒŒì¼ì´ ìˆì–´ì•¼ ì •ìƒ ì‘ë™

### ì£¼ìš” ì„¤ì • íŒŒë¼ë¯¸í„° ###
1) S3 ë²„í‚·
- S3 ë²„í‚·: bv-dropbox
- S3 read ê²½ë¡œ: êµí†µì¹´ë“œ/í†µê·¼OD/{YYYYMM}/{YYYYMM}_workod_purpose_transport_grid.parquet

2) ì—…ë¡œë“œ ëŒ€ìƒ DB ì •ë³´
- ì—…ë¡œë“œ DB: Postgres (dataops_test_181)                    << -- í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‹¤ì œ ìš´ì˜ DBë¡œ ë³€ê²½ í•„ìš”
- ìŠ¤í‚¤ë§ˆ: temporary
- í…Œì´ë¸”: tb_metropolitan_work_od (íŒŒí‹°ì…”ë‹: standard_ym)

3) ëŒ€ìƒ ì›”
- Airflow Variable: transportation_target_months í˜¹ì€ DAG Run Config ì˜ transportation_target_months íŒŒë¼ë¯¸í„° í™œìš©
- YYYYMM í˜•ì‹ì˜ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
- ë¯¸ì§€ì • ì‹œ ê¸°ë³¸ê°’: 202506

ìì„¸í•œ ë‚´ìš©ì€ 'êµí†µì¹´ë“œ_í†µê·¼OD_í›„ì²˜ë¦¬.md' ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
"""

from __future__ import annotations

import io, os
import pendulum
from typing import List, Dict
import logging
import s3fs
import boto3
import pandas as pd
import pyarrow.parquet as pq
import shapely.geometry as sg
from airflow.decorators import dag, task
from airflow.models import Variable
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.operators.python import PythonVirtualenvOperator
from botocore.exceptions import ClientError
from airflow.exceptions import AirflowSkipException
from shapely.geometry import Point

# -------------- ì „ì—­ ì„¤ì • --------------
# S3 ì„¤ì •
AWS_CONN_ID = 's3_bv_dropbox'
S3_BUCKET_NAME = 'bv-dropbox'

# ì—…ë¡œë“œí•  DB ì ‘ì† ì •ë³´
POSTGRES_CONN_ID = 'dn_airflow'
SCHEMA = 'transportation'

# ê·¸ë¦¬ë“œ ë³€í™˜ìš© ë§¤í•‘
X_MAPPING = list("ABCDEFG")
Y_MAPPING = list("BCDEFGH")

# DAG ì›” ë¯¸ì…ë ¥ ì‹œ ê¸°ë³¸ê°’
DEFAULT_MONTH = ["202501"] # , "202502", "202503", "202504", "202505", "202506"

# Helper í•¨ìˆ˜
def convert_grid_to_5179(grid_id: str) -> Point | None:
    """
    ë‹¨ì¼ ê·¸ë¦¬ë“œ ID -> Point ê°ì²´ ë³€í™˜
    """
    if not isinstance(grid_id, str) or len(grid_id) < 12:
        return None
    try:
        x_char, y_char = grid_id[0], grid_id[1]

        x_index = X_MAPPING.index(x_char)
        y_index = Y_MAPPING.index(y_char)

        x_tail = int(grid_id[2:7])
        y_tail = int(grid_id[7:])
    except (ValueError, IndexError):
        return None
    
    x = (x_index + 7) * 100_000 + x_tail
    y = (y_index + 14) * 100_000 + y_tail
    return Point(x, y)

def apply_grid_to_5179(series: pd.Series) -> pd.Series:
    return series.apply(convert_grid_to_5179)


# ---------- DAG ----------
@task.virtualenv(
    task_id="get_korean_business_day_task", 
    requirements=['pandas', 'pendulum', 'holidays'],
    system_site_packages=False)
def get_korean_business_day(ym:str) -> int:
    """
    YYYYMMí˜•íƒœì˜ ë¬¸ìì—´ì„ ë°›ì•„ í•´ë‹¹ ì›”ì˜ ì˜ì—…ì¼(í‰ì¼) ìˆ˜ë¥¼ ë°˜í™˜í•¨
    holidays ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ ê°€ìƒí™˜ê²½ì˜¤í¼ë ˆì´í„° í™œìš©
    """
    import datetime as dt
    import pandas as pd
    import calendar
    import holidays
    import logging
    logger = logging.getLogger('airflow.task')
    
    if len(ym)!=6 or not ym.isdigit():
        raise ValueError("YYYYMMí˜•íƒœì˜ ë¬¸ìì—´ì„ ì…ë ¥í•´ì•¼í•©ë‹ˆë‹¤.")
    year = int(ym[:4])
    month = int(ym[4:6])
    
    start_date = dt.date(year, month, 1)
    _, last_day = calendar.monthrange(year, month)
    end_date = dt.date(year, month, last_day)
    rng = pd.date_range(start=start_date, end=end_date, freq='D', tz='Asia/Seoul')
    
    is_weekday = rng.weekday < 5
    _KR_HOLIDAYS= holidays.KR(years=[year])
    holiday_ts = pd.Index([pd.Timestamp(d, tz='Asia/Seoul') for d in _KR_HOLIDAYS.keys()])
    is_holiday = rng.isin(holiday_ts)
    business_day_mask = is_weekday & ~is_holiday
    logger.debug(f"Calculated business days for {ym}: {business_day_mask.sum()}")
    
    return int(business_day_mask.sum())

@dag(
    dag_id="êµí†µì¹´ë“œ_í†µê·¼OD_í›„ì²˜ë¦¬",
    start_date=pendulum.datetime(2025, 11, 13, tz="Asia/Seoul"),
    schedule=None,          # ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •
    catchup=False,
    tags=["êµí†µì¹´ë“œ"],
    default_args={"owner": "ë¬¸ì¬ì‹"},
)

def transportation_data_postprocessing_2025():
    @task
    def resolve_target_months(**context) -> List[str]:
        """
        ì „ë‹¬ëœ ì›” ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
        """
        # 1) Run time config
        conf_months = context.get("dag_run").conf.get("transportation_target_months") if context.get("dag_run") else None

        if conf_months:
            if isinstance(conf_months, str):
                conf_months = [conf_months]
            return list(conf_months)
        # 2) Airflow Variables
        var_months = Variable.get("transportation_target_months", default_var=DEFAULT_MONTH)
        if isinstance(var_months, str):
            var_months = [m.strip() for m in var_months.split(",") if m.strip()]
        return list(var_months)
    

    @task
    def load_parquet_from_s3(ym: str) -> str:
        logger = logging.getLogger('airflow.task')
        s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
        src_key = f"êµí†µì¹´ë“œ/í†µê·¼OD/{ym}/{ym}_workod_purpose_transport_grid.parquet"

        s3_full_path = f"s3://{S3_BUCKET_NAME}/{src_key}"
        logger.info(f"Loading parquet from {s3_full_path}")

        try:
            # S3ì— íŒŒì¼ì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
            s3_hook.head_object(bucket_name=S3_BUCKET_NAME, key=src_key)
            
            logger.info(f"S3 object found. Path: {s3_full_path}")
            return s3_full_path

        except ClientError as e:
            error_code = e.response.get('Error', {}).get('Code')
            if error_code == 'NoSuchKey':
                logger.warning(f"S3 ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {s3_full_path}. ì´ íƒœìŠ¤í¬ë¥¼ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                raise AirflowSkipException(f"S3 ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {s3_full_path}")
            elif error_code == 'AccessDenied':
                logger.error(f"S3 ì ‘ê·¼ ê±°ë¶€: {s3_full_path}. IAM ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}")
                raise
            else:
                logger.exception(f"S3ì—ì„œ ê°ì²´ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ (ClientError): {e}")
                raise
        except Exception as e:
            logger.exception(f"Parquet íŒŒì¼ì„ S3ì—ì„œ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    @task
    def transform_parquet(s3_parquet_path: str, ym: str, business_days: int) -> str: # bytes -> str (íŒŒì¼ ê²½ë¡œ)
        """
        1) Parquet -> DataFrame
        2) ê·¸ë¦¬ë“œ -> 5179 ì¢Œí‘œ ë³€í™˜ -> wkt
        3) ì˜ë¬¸ ê·¸ë¦¬ë“œ -> êµ­ê°€í‘œì¤€í•œê¸€ê·¸ë¦¬ë“œ
        4) ì˜ì—…ì¼ ê¸°ë°˜ ì¼ í‰ê·  ì´ìš©ê±´ìˆ˜ ê³„ì‚° ë“±
        5) í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ê³¨ë¼ì„œ CSV ë¬¸ìì—´ìœ¼ë¡œ ë°˜í™˜
        """
        logger = logging.getLogger('airflow.task')
        logger.info(f"Transforming data from S3 path: {s3_parquet_path}")
        
        s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
        session = s3_hook.get_session()
        credentials = session.get_credentials()
        region_name = session.region_name
        fs_kwargs = {
            "key": credentials.access_key,
            "secret": credentials.secret_key,
            "token": credentials.token,
            "client_kwargs": {"region_name": region_name},
        }
        fs = s3fs.S3FileSystem(**fs_kwargs)

        # 1) Parquet -> DataFrame
        table = pq.read_table(s3_parquet_path, filesystem=fs)
        df = table.to_pandas()
        logger.info(f"DataFrame loaded. Shape: {df.shape}")

        # 2) ì¢Œí‘œ ë³€í™˜ (grid ID â†’ EPSG:5179 Point)
        vec_convert = pd.Series(df["ìŠ¹ì°¨ê·¸ë¦¬ë“œID"]).apply(convert_grid_to_5179)
        df["departure_station_geometry"] = vec_convert.apply(
            lambda pt: pt.wkt if pt else None
        )
        vec_convert = pd.Series(df["í•˜ì°¨ê·¸ë¦¬ë“œID"]).apply(convert_grid_to_5179)
        df["arrival_station_geometry"] = vec_convert.apply(
            lambda pt: pt.wkt if pt else None
        )

            
        # 3) ì¼í‰ê·  ì´ìš©ê±´ìˆ˜
        df['daily_use_count'] = (df['ì›”_ì´_í†µí–‰ëŸ‰'] / business_days).round(1)
        
        # 4) ëª©ì ëª…ì¹­ ë§¤í•‘
        df['purpose_name'] = df['ì¶œí‡´ê·¼êµ¬ë¶„'].map({'morning':'ì¶œê·¼', 'evening':'í‡´ê·¼'})
        # 5) ê¸°ì¤€ì—°ì›” ì»¬ëŸ¼ ì¶”ê°€
        df['standard_ym'] = ym
        # 6) ì˜ë¬¸ ê·¸ë¦¬ë“œ -> êµ­ê°€í‘œì¤€í•œê¸€ê·¸ë¦¬ë“œ ë³€í™˜
        grid_map = {'A':'ê°€', 'B':'ë‚˜', 'C':'ë‹¤', 'D':'ë¼', 'E':'ë§ˆ', 'F':'ë°”', 'G':'ì‚¬', 'H':'ì•„'}
        df['ìŠ¹ì°¨ê·¸ë¦¬ë“œID'] = df['ìŠ¹ì°¨ê·¸ë¦¬ë“œID'].replace(grid_map, regex=True)
        df['í•˜ì°¨ê·¸ë¦¬ë“œID'] = df['í•˜ì°¨ê·¸ë¦¬ë“œID'].replace(grid_map, regex=True)

        # 7) ì»¬ëŸ¼ëª… ì˜ë¬¸ í‘œì¤€í™”
        df_standardized = df.rename(columns={
            'ìŠ¹ì°¨ì •ë¥˜ì¥ID':'departure_station_id',
            'ìŠ¹ì°¨ì •ë¥˜ì¥ëª…ì¹­':'departure_station_name',
            'ìŠ¹ì°¨ì§€ì—­ì½”ë“œ':'departure_region_code',
            'ìŠ¹ì°¨êµí†µìˆ˜ë‹¨êµ¬ë¶„':'departure_station_type',
            'ìŠ¹ì°¨ê·¸ë¦¬ë“œID':'departure_grid_id',
            'í•˜ì°¨ì •ë¥˜ì¥ID':'arrival_station_id',
            'í•˜ì°¨ì •ë¥˜ì¥ëª…ì¹­':'arrival_station_name',
            'í•˜ì°¨ì§€ì—­ì½”ë“œ':'arrival_region_code',
            'í•˜ì°¨êµí†µìˆ˜ë‹¨êµ¬ë¶„':'arrival_station_type',
            'í•˜ì°¨ê·¸ë¦¬ë“œID':'arrival_grid_id',
            'íƒ‘ìŠ¹ì‹œê°„_ì¤‘ìœ„':'median_elapse_time',
            'íƒ‘ìŠ¹ì‹œê°„_í‰ê· ':'average_elapse_time',
            'ì´ë™ê±°ë¦¬_ì¤‘ìœ„':'median_distance',
            'ì´ë™ê±°ë¦¬_í‰ê· ':'average_distance',
        })
        db_cols = [
            "standard_ym",
            "departure_station_id",
            "departure_station_name",
            "departure_region_code",
            "departure_station_type",
            "departure_grid_id",
            "arrival_station_id",
            "arrival_station_name",
            "arrival_region_code",
            "arrival_station_type",
            "arrival_grid_id",
            "purpose_name",
            "daily_use_count",
            "median_elapse_time",
            "average_elapse_time",
            "median_distance",
            "average_distance",
            "departure_station_geometry",
            "arrival_station_geometry",
        ]
        df_standardized = df_standardized[db_cols]

        # 7) CSV ë²„í¼ì— WRITE 
        csv_buffer = io.StringIO()
        df_standardized.to_csv(csv_buffer, index=False, header=False, na_rep='\\N')
        csv_str = csv_buffer.getvalue()
        return csv_str

    @task
    def load_to_postgres(csv_str:str, ym:str) -> None:
        """
        `PostgresHook`ì„ ì´ìš©í•´ `COPY FROM STDIN` ìœ¼ë¡œ í•œ ë²ˆì— ì ì¬í•©ë‹ˆë‹¤.
        * í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìë™ ìƒì„± (ì²« ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ìˆ˜í–‰)
        """
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        logger = logging.getLogger('airflow.task')
        pg = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)

        # 1) í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±
        create_sql = f"""
        CREATE TABLE IF NOT EXISTS {SCHEMA}.tb_metropolitan_work_od (
            standard_ym                 VARCHAR(6) NOT NULL,
            departure_station_id        VARCHAR(20) NOT NULL,
            departure_station_name      VARCHAR(100),
            departure_region_code       VARCHAR(20) NOT NULL,
            departure_station_type      VARCHAR(20) NOT NULL,
            departure_grid_id           VARCHAR(12),
            arrival_station_id          VARCHAR(20) NOT NULL,
            arrival_station_name        VARCHAR(100),
            arrival_region_code         VARCHAR(20) NOT NULL,
            arrival_station_type        VARCHAR(20) NOT NULL,
            arrival_grid_id             VARCHAR(12),
            purpose_name                VARCHAR(10) NOT NULL,
            daily_use_count             NUMERIC(10,1),
            median_elapse_time          NUMERIC(5,0),
            average_elapse_time         NUMERIC(5,0),
            median_distance             NUMERIC(6,0),
            average_distance            NUMERIC(6,0),
            departure_station_geometry  GEOMETRY(Point, 5179),
            arrival_station_geometry    GEOMETRY(Point, 5179),
            -- PK ì¶”ê°€
            CONSTRAINT pk_metropolitan_workod 
            PRIMARY KEY (standard_ym, departure_station_id, departure_region_code, departure_station_type,
            arrival_station_id, arrival_region_code, arrival_station_type, purpose_name)
        )
        PARTITION BY LIST (standard_ym)
        ;
        """
        try:
            logger.info(f"Executing CREATE PARENT TABLE DDL for {SCHEMA}.tb_metropolitan_work_od")
            pg.run(create_sql)
        except Exception as e:
            logger.error(f"í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

        # íŒŒí‹°ì…˜ í…Œì´ë¸” ìƒì„±
        partition_sql = f"""
            CREATE TABLE IF NOT EXISTS {SCHEMA}.tb_metropolitan_work_od_{ym}
            PARTITION OF {SCHEMA}.tb_metropolitan_work_od
            FOR VALUES IN ('{ym}');
        """
        try:
            logger.info(f"Executing CREATE PARTITION TABLE DDL for {SCHEMA}.tb_metropolitan_work_od_{ym}")
            pg.run(partition_sql)
        except Exception as e:
            logger.error(f"íŒŒí‹°ì…˜ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
        truncate_sql = f"""
        TRUNCATE TABLE {SCHEMA}.tb_metropolitan_work_od_{ym};
        """
        try:
            logger.info(f"Executing TRUNCATE TABLE for {SCHEMA}.tb_metropolitan_work_od_{ym}")
            pg.run(truncate_sql)
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        
        # 2) COPY FROM STDIN ìœ¼ë¡œ ë°ì´í„° ì ì¬
        copy_sql = f"""
        COPY {SCHEMA}.tb_metropolitan_work_od (
            standard_ym,
            departure_station_id,
            departure_station_name,
            departure_region_code,
            departure_station_type,
            departure_grid_id,
            arrival_station_id,
            arrival_station_name,
            arrival_region_code,
            arrival_station_type,
            arrival_grid_id,
            purpose_name,
            daily_use_count,
            median_elapse_time,
            average_elapse_time,
            median_distance,
            average_distance,
            departure_station_geometry,
            arrival_station_geometry
        )
        FROM STDIN WITH (FORMAT csv, NULL '\\N');
        """
        with pg.get_conn() as conn:
            with conn.cursor() as cur:
                cur.copy_expert(sql=copy_sql, file=io.StringIO(csv_str))
        

    @task
    def final_success(**context):
        context["ti"].log.info("ğŸ‰ëª¨ë“  ì›”ì— ëŒ€í•´ ì„±ê³µì ìœ¼ë¡œ ì ì¬ë˜ì—ˆìŠµë‹ˆë‹¤.ğŸ‰")
        return "PROCESSING_SUCCESS"

    # ì›Œí¬í”Œë¡œìš° ì •ì˜
    month_list = resolve_target_months()
    business_days = get_korean_business_day.expand(ym=month_list)
    
    s3_parquet_path = load_parquet_from_s3.expand(ym=month_list)
    csv_strings = transform_parquet.expand(s3_parquet_path=s3_parquet_path, ym=month_list, business_days=business_days)
    last_load_tasks = load_to_postgres.expand(csv_str=csv_strings, ym=month_list)
    
    last_load_tasks >> final_success()


# dag ë“±ë¡
transportation_data_postprocessing_2025()